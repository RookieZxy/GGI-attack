{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d441f53",
   "metadata": {},
   "source": [
    "## Example of Jailbreaking LLaMA-2-chat-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f682193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9a36e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import copy\n",
    "\n",
    "from llm_attacks.minimal_gcg.opt_utils import token_gradients, sample_control, get_logits, target_loss, get_logits_in_batches\n",
    "from llm_attacks.minimal_gcg.opt_utils import load_model_and_tokenizer, get_filtered_cands\n",
    "from llm_attacks.minimal_gcg.string_utils import SuffixManager, load_conversation_template\n",
    "from llm_attacks import get_nonascii_toks\n",
    "\n",
    "from livelossplot import PlotLosses # pip install livelossplot\n",
    "from livelossplot.outputs import MatplotlibPlot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the random seed for NumPy\n",
    "np.random.seed(20)\n",
    "\n",
    "# Set the random seed for PyTorch\n",
    "torch.manual_seed(20)\n",
    "\n",
    "# If you are using CUDA (i.e., a GPU), also set the seed for it\n",
    "torch.cuda.manual_seed_all(20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24966e66",
   "metadata": {},
   "source": [
    "### Experiment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2f64c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'meta-llama/Llama-2-7b-chat-hf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b08803",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM, GenerationConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    use_fast=False,\n",
    "    token='your token'\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                torch_dtype=torch.float16,\n",
    "                trust_remote_code=True,\n",
    "                load_in_8bit=True,\n",
    "                low_cpu_mem_usage=True, \n",
    "                use_cache=False,\n",
    "                token='your token'\n",
    "            ).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f239ef55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"Baidicoot/augmented_advbench_v4\")['train']\n",
    "train_set1 = dataset.select(range(10,20))\n",
    "test_set = dataset.select(range(20,len(dataset['rejected'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f9911c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = [train_set1[0],train_set1[4],train_set1[7],train_set1[9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90220820",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_adv_tokens = 2\n",
    "num_shots = 4\n",
    "num_queries = 4\n",
    "num_steps = 100\n",
    "\n",
    "batch_size = 32\n",
    "topk = 64\n",
    "\n",
    "original_num_shots = num_shots\n",
    "\n",
    "adv_prompts_init = [' a b' for i in range(num_shots)][:num_shots]\n",
    "\n",
    "\n",
    "demos8 = train_set[:num_shots]\n",
    "queries2 = train_set1.select(range(num_queries))\n",
    "\n",
    "\n",
    "demos = []\n",
    "queries = []\n",
    "target = []\n",
    "real_answers = []\n",
    "queries_questions = []\n",
    "\n",
    "instruction = \"Please answer the following questions step by step.\"\n",
    "\n",
    "adv_answer = 'Sure, here'\n",
    "\n",
    "for index in range(len(demos8)):\n",
    "    single_question,single_query = demos8[index]['prompt'],demos8[index]['rejected']\n",
    "    firstpart = '\\nQuestion: '+ single_question + '\\nAnswer:'\n",
    "    if('sure' not in single_query.lower()):\n",
    "        single_query = 'Sure, '+single_query\n",
    "\n",
    "    secondpart = single_query.split(' ')\n",
    "    demos.append({'sentence': firstpart+' '+secondpart[0], 'label': ' '+' '.join(secondpart[1:25])})\n",
    "\n",
    "for index in range(num_queries):\n",
    "    single_question,single_query = queries2[index]['prompt'],queries2[index]['rejected']\n",
    "    newsentence = single_query.split('\\n')\n",
    "    firstpart = '\\nQuestion: '+ single_question.strip()\n",
    "    secondpart = '\\nAnswer:'\n",
    "    queries.append(firstpart+secondpart)\n",
    "    queries_questions.append(firstpart+secondpart)\n",
    "    target.append(' '+adv_answer)\n",
    "\n",
    "allow_non_ascii = False # you can set this to True to use unicode tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d3a6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "class SuffixManager:\n",
    "    def __init__(self, *, model_name, tokenizer, demos, queries, instruction,\n",
    "                 targets, adv_prompts, num_adv_tokens): \n",
    "                 \n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = tokenizer\n",
    "        self.demos = demos\n",
    "        self.queries = queries\n",
    "        self.instruction = instruction\n",
    "        self.targets = targets\n",
    "        self.adv_prompts = adv_prompts\n",
    "        self.num_adv_tokens = num_adv_tokens\n",
    "        self._demos_slice = [[] for i in range(len(demos))]\n",
    "        self._control_slice = [[] for i in range(len(demos))]\n",
    "        self._demos_label_slice = [[] for i in range(len(demos))]\n",
    "        self._queries_slice = [[] for i in range(len(queries))]\n",
    "        self._target_slice = [[] for i in range(len(queries))]\n",
    "        self._loss_slice = [[] for i in range(len(queries))]\n",
    "    \n",
    "    def get_prompt(self, adv_prompts=None):\n",
    "        if adv_prompts is not None:\n",
    "            self.adv_prompts = adv_prompts\n",
    "        \n",
    "        prompts = []\n",
    "        input = \"\"\n",
    "        # i is single query, j is single target\n",
    "        for i, j  in zip(self.queries ,self.targets):\n",
    "            newstr = ''\n",
    "            for x, y in zip(self.demos, self.adv_prompts):\n",
    "                if(y!='' and y[0]!=' '):\n",
    "                    newstr += (x['sentence'] + ' ' + y + x['label'])\n",
    "                else:\n",
    "                    newstr += (x['sentence'] + y + x['label'])\n",
    "            prompts.append(self.instruction+newstr+i+j)\n",
    "\n",
    "        input += self.instruction\n",
    "        toks = self.tokenizer(input).input_ids\n",
    "        self._instruction_slice = slice(None, len(toks))\n",
    "        # demos and labels position\n",
    "        for index, (demo, adv_prompt) in enumerate(zip(self.demos, self.adv_prompts)):\n",
    "            input += demo['sentence']#+ 'Answer:' \n",
    "            toks = self.tokenizer(input).input_ids\n",
    "            self._demos_slice[index] = slice(self._instruction_slice.stop, len(toks))\n",
    "            if(adv_prompt!='' and adv_prompt[0]!=' '):\n",
    "                input += (' '+adv_prompt)\n",
    "            else:\n",
    "                input += adv_prompt\n",
    "            toks = self.tokenizer(input).input_ids\n",
    "\n",
    "\n",
    "            if self._demos_slice[index].stop + self.num_adv_tokens != len(toks):\n",
    "                self._control_slice[index] = slice(self._demos_slice[index].stop-1, len(toks))\n",
    "            else:\n",
    "                self._control_slice[index] = slice(self._demos_slice[index].stop, len(toks))\n",
    "            input += demo['label']\n",
    "            toks = self.tokenizer(input).input_ids\n",
    "            self._demos_label_slice[index] = slice(self._control_slice[index].stop, len(toks))\n",
    "\n",
    "        # query position\n",
    "        for index, (query, target) in enumerate(zip(self.queries, self.targets)):\n",
    "            temp_input = copy.deepcopy(input)\n",
    "            temp_input += query\n",
    "            toks = self.tokenizer(temp_input).input_ids\n",
    "            self._queries_slice[index] = slice(self._demos_label_slice[-1].stop, len(toks))\n",
    "            temp_input += target\n",
    "            toks = self.tokenizer(temp_input).input_ids\n",
    "            self._target_slice[index] = slice(self._queries_slice[index].stop, len(toks))\n",
    "            self._loss_slice[index] = slice(self._queries_slice[index].stop-1, len(toks)-1)\n",
    "\n",
    "        return prompts\n",
    "\n",
    "    \n",
    "    def get_input_ids(self, adv_prompts=None):\n",
    "\n",
    "        prompt_list = self.get_prompt(adv_prompts=adv_prompts)\n",
    "        input_ids_list = []\n",
    "        for prompt, target_slice in zip(prompt_list, self._target_slice):\n",
    "            toks = self.tokenizer(prompt).input_ids\n",
    "\n",
    "            input_ids_list.append(torch.tensor(toks[:target_slice.stop]))\n",
    "        return input_ids_list\n",
    "    \n",
    "    def get_input_ids_output(self, adv_prompts=None):\n",
    "\n",
    "        prompt_list = self.get_prompt(adv_prompts=adv_prompts)\n",
    "        input_ids_list = []\n",
    "        for prompt, target_slice in zip(prompt_list, self._target_slice):\n",
    "            toks = self.tokenizer(prompt).input_ids\n",
    "            input_ids_list.append(torch.tensor(toks[:target_slice.start]))\n",
    "        return input_ids_list\n",
    "    \n",
    "    def get_prompt_ids(self, adv_prompts=None):\n",
    "\n",
    "        prompt_list = self.get_prompt(adv_prompts=adv_prompts)\n",
    "        toks = self.tokenizer(prompt_list[0]).input_ids\n",
    "        input_ids_list = torch.tensor(toks[:self._queries_slice[0].start])\n",
    "        return input_ids_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a4d683",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_manager = SuffixManager(model_name=model_name,\n",
    "                                tokenizer=tokenizer, \n",
    "                                demos=demos, \n",
    "                                queries=queries, \n",
    "                                instruction=instruction, \n",
    "                                targets=target, \n",
    "                                adv_prompts=adv_prompts_init,\n",
    "                                num_adv_tokens=num_adv_tokens,\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cc58e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_allowed_tokens = None if allow_non_ascii else get_nonascii_toks(tokenizer) \n",
    "adv_suffix = adv_prompts_init\n",
    "best_loss = math.inf\n",
    "best_adv_suffix = None\n",
    "losses_list = []\n",
    "\n",
    "input_ids_list = suffix_manager.get_input_ids(adv_prompts=adv_prompts_init)\n",
    "input_ids_list = [input_ids for input_ids in input_ids_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938fb94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logits_in_batches(model, tokenizer, input_ids_list, control_slice_list, test_controls, batch_size, return_ids=True, num_adv_tokens=2, num_shots=None, target_slice= None):\n",
    "    total_losses = []\n",
    "\n",
    "    # Number of batches\n",
    "    num_batches = (len(test_controls) + batch_size - 1) // batch_size\n",
    "\n",
    "\n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "\n",
    "        # Extract the current batch of input_ids\n",
    "        batch_test_controls = test_controls[start_idx:end_idx]\n",
    "\n",
    "        # Call get_logits for this batch\n",
    "        logits, ids = get_logits(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            input_ids_list=input_ids_list,\n",
    "            control_slice_list=control_slice_list,\n",
    "            test_controls=batch_test_controls,\n",
    "            return_ids=return_ids,\n",
    "            batch_size=batch_size,  # This may or may not be needed depending on how `get_logits` is implemented\n",
    "            num_adv_tokens=num_adv_tokens,\n",
    "            num_shots=num_shots\n",
    "        )\n",
    "\n",
    "\n",
    "        # losses = target_loss_new(logits, ids, target_slice)\n",
    "        losses = target_loss(logits, ids, target_slice)\n",
    "        for loss in losses:\n",
    "            total_losses += [loss.item()]\n",
    "        logits = None\n",
    "        ids = None\n",
    "        losses=None\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return total_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a565121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filtered_cands(tokenizer,filter_cand, control_cand, model_name, num_tokens):\n",
    "    cands, count = [], 0\n",
    "    for i in range(len(control_cand)):\n",
    "        valid = True\n",
    "        # decoded_str = tokenizer.decode(control_cand[i], skip_special_tokens=True)\n",
    "        if filter_cand:\n",
    "            for token in control_cand[i]:\n",
    "                token_ids = torch.tensor(tokenizer(token, add_special_tokens=False).input_ids)\n",
    "                if token_ids.size()[0] != num_tokens:\n",
    "                    valid = False\n",
    "            if valid:\n",
    "                cands.append(control_cand[i])\n",
    "            else:\n",
    "                count += 1\n",
    "        else:\n",
    "            cands.append(control_cand[i])\n",
    "\n",
    "    if filter_cand:\n",
    "        cands = cands + [cands[-1]] * (len(control_cand) - len(cands))\n",
    "        print(f\"Warning: {round(count / len(control_cand), 2)} control candidates were not valid\")\n",
    "    return cands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b1d9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bee3bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'directory' \n",
    "plotlosses = PlotLosses(outputs=[MatplotlibPlot(figpath =filename)])\n",
    "\n",
    "# get candidate token list\n",
    "not_allowed_tokens = None if allow_non_ascii else get_nonascii_toks(tokenizer) \n",
    "adv_suffix = adv_prompts_init\n",
    "best_loss = math.inf\n",
    "best_adv_suffix = None\n",
    "losses_list = []\n",
    "\n",
    "\n",
    "for i in range(num_steps):\n",
    "    input_ids_list = suffix_manager.get_input_ids(adv_prompts=adv_suffix)\n",
    "    input_ids_list = [input_ids for input_ids in input_ids_list]\n",
    "    coordinate_grad = token_gradients(model, \n",
    "                input_ids_list, \n",
    "                suffix_manager._control_slice, \n",
    "                suffix_manager._target_slice, \n",
    "                suffix_manager._loss_slice,\n",
    "                )\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        adv_suffix_tokens = None\n",
    "        for _control_slice in suffix_manager._control_slice:\n",
    "            # print(_control_slice)\n",
    "            if adv_suffix_tokens == None:\n",
    "                adv_suffix_tokens = input_ids_list[0][_control_slice] \n",
    "            else: \n",
    "                adv_suffix_tokens = torch.cat([ adv_suffix_tokens, input_ids_list[0][_control_slice]])\n",
    "\n",
    "        new_adv_suffix_toks = sample_control(adv_suffix_tokens, \n",
    "                        coordinate_grad, \n",
    "                        batch_size, \n",
    "                        topk=topk, \n",
    "                        temp=1, \n",
    "                        not_allowed_tokens=not_allowed_tokens)\n",
    "        \n",
    "\n",
    "        new_adv_suffix = []\n",
    "        for index, each_toks in enumerate(new_adv_suffix_toks):\n",
    "            new_adv_suffix.append([])\n",
    "            for i in range(num_shots):\n",
    "                new_adv_suffix[index].append(tokenizer.decode(each_toks[i*num_adv_tokens:i*num_adv_tokens+num_adv_tokens]))\n",
    "\n",
    "        new_adv_suffix = get_filtered_cands(tokenizer, \n",
    "            filter_cand=True, \n",
    "            control_cand=new_adv_suffix,\n",
    "            model_name=model_name,\n",
    "            num_tokens=num_adv_tokens)\n",
    "    \n",
    "        losses = get_logits_in_batches(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            input_ids_list=input_ids_list,\n",
    "            control_slice_list=suffix_manager._control_slice,\n",
    "            test_controls=new_adv_suffix,\n",
    "            batch_size=64,\n",
    "            num_adv_tokens=num_adv_tokens,\n",
    "            num_shots=num_shots,\n",
    "            target_slice = suffix_manager._target_slice\n",
    "        )\n",
    "\n",
    "        min_loss = min(losses)\n",
    "\n",
    "        best_new_adv_suffix_id = losses.index(min_loss)\n",
    "\n",
    "        best_new_adv_suffix = new_adv_suffix[best_new_adv_suffix_id]\n",
    "\n",
    "        current_loss = losses[best_new_adv_suffix_id]\n",
    "        losses_list.append(current_loss)\n",
    "\n",
    "        best_new_adv_suffix_ids = tokenizer(best_new_adv_suffix , add_special_tokens=False).input_ids\n",
    "\n",
    "        adv_suffix = best_new_adv_suffix\n",
    "\n",
    "\n",
    "    if len(adv_suffix) != num_shots:\n",
    "        print(adv_suffix)\n",
    "        print(\"error\")\n",
    "        # print(adv_suffix)\n",
    "        break\n",
    "    # Create a dynamic plot for the loss.\n",
    "    plotlosses.update({'Loss': current_loss})\n",
    "    plotlosses.send() \n",
    "    print(adv_suffix)\n",
    "    if current_loss < best_loss:\n",
    "        best_loss = current_loss\n",
    "        best_adv_suffix = adv_suffix\n",
    "    \n",
    "    print(f\"\\nCurrent Suffix:{adv_suffix}\", end='\\r')\n",
    "    \n",
    "    \n",
    "    del coordinate_grad, adv_suffix_tokens ; gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0084b504",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_loss) \n",
    "print(best_adv_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6bc494",
   "metadata": {},
   "outputs": [],
   "source": [
    "demos = []\n",
    "queries = []\n",
    "demos_un = \"\"\n",
    "for index in range(len(demos8)):\n",
    "    single_question,single_query = demos8[index]['prompt'],demos8[index]['rejected']\n",
    "    firstpart = '\\nQuestion: '+ single_question + '\\nAnswer:'\n",
    "    if('sure' not in single_query.lower()):\n",
    "        single_query = 'Sure, '+single_query\n",
    "\n",
    "    secondpart = single_query.split(' ')\n",
    "    demos.append({'sentence': firstpart+' '+secondpart[0], 'label': ' '+' '.join(secondpart[1:100])})\n",
    "    demos_un += firstpart+' '+' '.join(secondpart[0:100])\n",
    "\n",
    "quer = []\n",
    "questions = []\n",
    "i = 0\n",
    "for index in range(len(test_set['prompt'])):\n",
    "    i+=1\n",
    "    if(i==101):\n",
    "        break\n",
    "    single_question,single_query = test_set[index]['prompt'],test_set[index]['rejected']\n",
    "    newsentence = single_query.split('\\n')\n",
    "    firstpart = '\\nQuestion: '+ single_question.strip()\n",
    "    secondpart = '\\nAnswer:'\n",
    "    questions.append(firstpart)\n",
    "    queries.append(firstpart+secondpart)\n",
    "    queries_questions.append(firstpart+secondpart)\n",
    "    quer.append(demos_un+firstpart+secondpart)\n",
    "    target.append(' '+adv_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44df41cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_manager = SuffixManager(model_name=model_name,\n",
    "                                tokenizer=tokenizer, \n",
    "                                demos=demos, \n",
    "                                queries=queries, \n",
    "                                instruction=instruction, \n",
    "                                targets=target, \n",
    "                                adv_prompts=best_adv_suffix,\n",
    "                                num_adv_tokens=num_adv_tokens,\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb1e832",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = suffix_manager.get_input_ids_output(adv_prompts=best_adv_suffix)\n",
    "tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d3d9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(output)):\n",
    "    outputs = model.generate((output[i].unsqueeze(0)).to(model.device), max_new_tokens=100,return_dict_in_generate=True, output_scores=True)\n",
    "    print(questions[i])\n",
    "    print(tokenizer.decode(outputs['sequences'][0][len(output[i]):]))\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64634ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(quer)):\n",
    "    output1 = instruction+quer[i]\n",
    "    input_tensor = tokenizer(output1, return_tensors=\"pt\",add_special_tokens=True)\n",
    "    outputs = model.generate(input_tensor['input_ids'].to(model.device), max_new_tokens=100,return_dict_in_generate=True, output_scores=True)\n",
    "    print(questions[i])\n",
    "    print(tokenizer.decode(outputs['sequences'][0][input_tensor['input_ids'].shape[-1]:]))\n",
    "    print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
